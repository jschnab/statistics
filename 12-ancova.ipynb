{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison of regression lines and analysis of covariance\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Previously, we discussed simple linear regression which involves fitting a straight line to a number of points using the method of least squares. In this chapter we show how to compare two or more regression lines. We also show how this technique can be used with analysis of variance to control experimental error, a method called analysis of covariance.\n",
    "\n",
    "## 2. Comparison of two regression lines\n",
    "\n",
    "Here we describe how to test whether regression lines fitted to two independent sets of data are parallel. Using the two independent estimates of slope $\\hat{\\beta_1}$, $\\hat{\\beta_2}$ based on $n_1$ and $n_2$ observations, you wish to test the hypothesis that $\\hat{\\beta_1} = \\hat{\\beta_2}$. If you can assume parallel lines you can go on to test whether the intercepts are equal. If they are you can fit an overall line to the data.\n",
    "\n",
    "In our working example, a greenhouse experiment was carried out to investigate the response of strawberry to two types of fertiliser (F1 and F2). Four plants were grown in each pot and twelve pots were treated at random with each fertiliser. At harvest time various measurements were made on each pot, but for this example we are only interested in the total fruit dry weight (X) and the total leaf area(Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import t, f, probplot, linregress, levene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe of experimental data\n",
    "data = pd.DataFrame({'fertiliser':[1 for _ in range(12)] + [2 for _ in range(12)],\n",
    "                     'X':[0.29, 0.43, 0.21, 0.53, 0.27, 0.33, 0.47, 0.40, 0.48, 0.30, 0.37, 0.30,\n",
    "                          0.27, 0.37, 0.42, 0.19, 0.30, 0.25, 0.35, 0.48, 0.22, 0.30, 0.14, 0.32],\n",
    "                     'Y':[144, 180, 60, 226, 105, 111, 217, 221, 218, 137, 153, 105,\n",
    "                          129, 206, 172, 80, 124, 89, 134, 220, 138, 105, 62, 181]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Testing for equality of slopes\n",
    "\n",
    "To test the hypothesis that $\\hat{\\beta_1} = \\hat{\\beta_2}$ carry out a t-test by calculating $t = (\\hat{\\beta_1} - \\hat{\\beta_2}) \\: / \\: SED$ and calculate a p-value on $n_1 + n_2 - 4$ degrees of freedom where SED is the standard error of the difference between the two fitted slopes.\n",
    "\n",
    "$$ SED = \\sqrt{s_p^2 \\left( \\frac{1}{S_{xx1}} + \\frac{1}{S_{xx2}} \\right)} $$\n",
    "\n",
    "$$ s_p^2 = \\frac{ResidSS_1 + ResidSS_2}{(n_1 - 2) + (n_2 - 2)} $$\n",
    "\n",
    "$s_p^2$ is an estimate of the assumed common population residual variance. When $n_1 = n_2$ this is the average of the two residual mean squares $RMS_1$ and $RMS_2$. To test the assumption of common population variance you divide the larger RMS by the smaller to obtain an F-value and then calculate a p-value on $n - 2$ degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get x and y values for the two treatments\n",
    "X1 = data[data['fertiliser']==1]['X']\n",
    "X2 = data[data['fertiliser']==2]['X']\n",
    "Y1 = data[data['fertiliser']==1]['Y']\n",
    "Y2 = data[data['fertiliser']==2]['Y']\n",
    "\n",
    "#sample size\n",
    "n1 = len(data[data['fertiliser']==1])\n",
    "n2 = len(data[data['fertiliser']==2])\n",
    "\n",
    "#degrees of freedom\n",
    "df1 = n1 - 2\n",
    "df2 = n2 - 2\n",
    "df_resid = df1 + df2\n",
    "\n",
    "#get sum of squares\n",
    "Sxx1 = X1.var() * (n1 - 1)\n",
    "Sxx2 = X2.var() * (n2 - 1)\n",
    "\n",
    "#perform  linear regression\n",
    "slope1, intercept1, rval1, pval1, stderr1 = linregress(X1, Y1)\n",
    "slope2, intercept2, rval2, pval2, stderr2 = linregress(X2, Y2)\n",
    "\n",
    "#fitted values\n",
    "Y1_fit = X1 * slope1 + intercept1\n",
    "Y2_fit = X2 * slope2 + intercept2\n",
    "\n",
    "#residual sum of squares\n",
    "resid_ss1 = np.sum((Y1 - Y1_fit) ** 2)\n",
    "resid_ss2 = np.sum((Y2 - Y2_fit) ** 2)\n",
    "\n",
    "#residual mean squares\n",
    "resid_ms1 = resid_ss1 / df1\n",
    "resid_ms2 = resid_ss2 / df2\n",
    "\n",
    "#common population residual variance\n",
    "sp2 = (resid_ss1 + resid_ss2) / (n1 + n2 - 4)\n",
    "\n",
    "#standard error of the difference between the slopes\n",
    "sed = np.sqrt(sp2 * (1 / Sxx1 + 1 / Sxx2))\n",
    "\n",
    "t_val = (slope1 - slope2) / sed\n",
    "\n",
    "p_val = 1 - t.cdf(abs(t_val), df_resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value for the test that the slopes are equal is 0.22 so we cannot reject this hypothesis. The data are thus in agreement with the hypothesis of a common slope. The estimate for the common slope is:\n",
    "\n",
    "$$  \\hat{\\beta} = \\frac{S_{xy1} + S_{xy2}}{S_{xx1} + S_{xx2}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(x, y):\n",
    "    \"\"\"Perform a linear regression of Y on X\"\"\"\n",
    "    #corrected sum of products Sxy\n",
    "    Sxy = np.sum(x * y) - np.sum(x) * np.sum(y) / len(x)\n",
    "\n",
    "    #corrected sum of squares of X\n",
    "    Sxx = np.sum(x ** 2) - np.sum(x) ** 2 / len(x)\n",
    "\n",
    "    #calculation of the parameters of the regression line y = a + b * x\n",
    "    b = Sxy / Sxx\n",
    "    a = np.mean(y) - b * np.mean(x)\n",
    "    \n",
    "    #calculate fitted y values\n",
    "    y_hat = np.array([a + b * i for i in x])\n",
    "    \n",
    "    #calculation of residuals\n",
    "    resid = np.zeros(len(y))\n",
    "    for i in range(len(y)):\n",
    "        resid[i] = y[i] - y_hat[i]\n",
    "\n",
    "    print(\"Verification of the assumptions of the linear regression\\n\")\n",
    "    #plotting of residuals\n",
    "    fig, ((ax0, ax1), (ax2, _)) = plt.subplots(nrows=2, ncols=2)\n",
    "    fig.delaxes(_)\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.8)\n",
    "    ax0.hist(resid)\n",
    "    ax0.set_xlabel(\"Residuals\")\n",
    "    ax0.set_ylabel(\"Frequency\")\n",
    "    ax0.set_title(\"Histogram of residuals\")\n",
    "    probplot(resid, plot=ax1)\n",
    "    ax1.set_xlabel(\"Normal score\")\n",
    "    ax1.set_ylabel(\"Residuals\")\n",
    "    ax1.set_title(\"Q-Q plot of residuals\")\n",
    "    ax2.scatter(y_hat, resid)\n",
    "    ax2.set_xlabel(\"Fitted values\")\n",
    "    ax2.set_ylabel(\"Residuals\")\n",
    "    ax2.set_title(\"Residuals VS fitted values\")\n",
    "    ax2.axhline(y=0, linewidth=0.5, color=\"grey\")\n",
    "    plt.show()\n",
    "    \n",
    "    #calculation of R-sq\n",
    "    SSE = np.sum((y - y_hat) ** 2)\n",
    "    SSR = np.sum((y_hat - np.mean(y)) ** 2)\n",
    "    Rsq = SSR / (SSE + SSR)\n",
    "    \n",
    "    #calculation of adjusted R-sq\n",
    "    #total mean square\n",
    "    tms = np.sum((y - np.mean(y)) ** 2) / (len(y) - 1)\n",
    "    Rsq_adj = (tms - SSE / (len(y) - 2)) / tms\n",
    "    \n",
    "    #testing null hypothesis that b = 0\n",
    "    #degrees of freedom of MSE\n",
    "    dfd = len(y) - 2\n",
    "    MSE = SSE / dfd\n",
    "    #calculation of the standard error of b\n",
    "    SE_b = sqrt(MSE / Sxx)\n",
    "    #calculation of t-value\n",
    "    t_val_b =  b / SE_b\n",
    "    #calculation of p-value\n",
    "    p_val_b = 2 * (1 - t.cdf(abs(t_val_b), df=dfd))\n",
    "    \n",
    "    #testing the significance of the regression (analysis of variance)\n",
    "    #degrees of freedom of MSR\n",
    "    dfn = 1\n",
    "    #variance ratio\n",
    "    MSR = SSR / 1\n",
    "    vr = MSR / MSE\n",
    "    #p-value\n",
    "    p_val_f = 1 - f.cdf(vr, dfn, dfd)\n",
    "    \n",
    "    #test whether intercept is significantly different from zero\n",
    "    SE_a = sqrt(MSE * (1 / len(y) + np.mean(x) ** 2 / Sxx))\n",
    "    t_val_a = a / SE_a\n",
    "    p_val_a = 2 * (1 - t.cdf(abs(t_val_a), df=dfd))\n",
    "    \n",
    "    #calculate confidence interval values for fitted Y values\n",
    "    x_val = np.linspace(min(x), max(x), 200)\n",
    "    y_fit = np.array([a + b * i for i in x_val])\n",
    "    ci_lo = np.zeros(len(x_val))\n",
    "    ci_hi = np.zeros(len(x_val))\n",
    "    for i in range(len(x_val)):\n",
    "        ci_lo[i] = y_fit[i] + t.ppf(0.025, df=len(y) - 2) * sqrt(MSE * (1 / len(y) + (x_val[i] - np.mean(x)) ** 2 / Sxx))\n",
    "        ci_hi[i] = y_fit[i] + t.ppf(0.975, df=len(y) - 2) * sqrt(MSE * (1 / len(y) + (x_val[i] - np.mean(x)) ** 2 / Sxx))\n",
    "\n",
    "    #plot linear regression with confidence interval on fitted Y values\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x, y, color=\"C0\", label=\"experimental points\")\n",
    "    ax.plot(x, y_hat, color=\"C3\", label=\"regression line\")\n",
    "    ax.plot(x_val, ci_lo, color=\"black\", linestyle=\"--\", linewidth=0.5, label=\"95% CI\")\n",
    "    ax.plot(x_val, ci_hi, color=\"black\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_title(\"Linear regression of y on x\")\n",
    "    #ax.set_xlim(-1, 101)\n",
    "    #ax.set_ylim(-1, 22)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"The regression equation is\")\n",
    "    print(\"Y = {0:.3f} {1:+.3f} * X\\n\".format(a, b))\n",
    "    print(\"{0:11}{1:>10}{2:>10}{3:>10}{4:>10}\".format(\"\", \"Coef\", \"StDev\", \"t-value\", \"p-value\"))\n",
    "    print(\"{0:11}{1:10.4f}{2:10.4f}{3:10.4f}{4:10.5f}\".format(\"Intercept\", a, SE_a, t_val_a, p_val_a))\n",
    "    print(\"{0:11}{1:10.4f}{2:10.4f}{3:10.4f}{4:10.5f}\\n\".format(\"Slope\", b, SE_b, t_val_b, p_val_b))\n",
    "    print(\"R-sq = {0:.1f}%, R-sq(adj) = {1:.1f}%\\n\".format(Rsq*100, Rsq_adj*100))\n",
    "    print(\"Analysis of variance\")\n",
    "    print(\"{0:15}{1:>10}{2:>10}{3:>10}{4:>10}{5:>10}\".format(\"Source\", \"df\", \"Sum sq\", \"Mean sq\", \"F-value\", \"p-value\"))\n",
    "    print(\"{0:15}{1:10}{2:10.2f}{3:10.2f}{4:10.3f}{5:10.5f}\".format(\"Regression\", 1, SSR, MSR, vr, p_val_f))\n",
    "    print(\"{0:15}{1:10}{2:10.2f}{3:10.2f}\".format(\"Residual error\", dfd, SSE, MSE))\n",
    "    print(\"{0:15}{1:10}{2:10.2f}\\n\".format(\"Total\", dfd + 1, SSR + SSE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
